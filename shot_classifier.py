# -*- coding: utf-8 -*-
"""shot_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H8L69KFArgS19hAUwrJIzZfE7oV9rJV2
"""

import os
import pandas as pd
import random

# dataset_dir = "/content/drive/MyDrive/CricShot10 dataset"

# data = []

# for shot_type in os.listdir(dataset_dir):
#     shot_dir = os.path.join(dataset_dir, shot_type)
#     if os.path.isdir(shot_dir):
#         for video_file in os.listdir(shot_dir):
#             if video_file.endswith('.avi'):  # Assuming video files are in .mp4 format
#                 video_path = os.path.join(shot_dir, video_file)
#                 data.append([video_path, shot_type])


# df = pd.DataFrame(data, columns=['video_path', 'label'])

from google.colab import drive
drive.mount('/content/drive')

#df.head()

# csv_path = "/content/drive/MyDrive/Sports Analysis/video_dataset.csv"
# df.to_csv(csv_path, index=False)

# print(f"CSV file saved at {csv_path}")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import cv2
import numpy as np

from sklearn.model_selection import train_test_split

# Load the CSV file
df = pd.read_csv('/content/drive/MyDrive/Sports Analysis/video_dataset.csv')

# df = df.groupby('label').sample(n=100, random_state=42)
# Split data (70% train, 15% validation, 15% test)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
# val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

# Save the splits as CSVs (optional)
# train_df.to_csv('/content/drive/MyDrive/your_project_folder/train_data.csv', index=False)
# val_df.to_csv('/content/drive/MyDrive/your_project_folder/val_data.csv', index=False)
# test_df.to_csv('/content/drive/MyDrive/your_project_folder/test_data.csv', index=False)

print(f"Train size: {len(train_df)}, Test size: {len(test_df)}")

label_mapping = {label: idx for idx, label in enumerate(df['label'].unique())}
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

label_mapping

import torch
from torch.utils.data import DataLoader
from transformers import VideoMAEForVideoClassification, AdamW
from tqdm import tqdm

num_epochs = 10
learning_rate = 1e-4
batch_size = 8

"""##Video MAE"""

model = VideoMAEForVideoClassification.from_pretrained("MCG-NJU/videomae-base")

num_features = model.classifier.in_features
model.classifier = torch.nn.Sequential(
    torch.nn.Linear(num_features, 512),
    torch.nn.ReLU(),
    torch.nn.Linear(512, len(label_mapping))
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = AdamW(model.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

class VideoDataset(Dataset):
    def __init__(self, dataframe, transform=None, num_frames=30):
        self.dataframe = dataframe
        self.transform = transform
        self.num_frames = num_frames

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        video_path = self.dataframe.iloc[idx, 0]
        label = self.dataframe.iloc[idx, 1]


        label = label_mapping[label]


        cap = cv2.VideoCapture(video_path)
        frames = []
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (224, 224))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        cap.release()


        frames = np.array(frames).astype(np.float32) / 255.0

        frames = torch.Tensor(frames).permute(0, 3, 1, 2)

        num_video_frames = frames.shape[0]

        if num_video_frames > self.num_frames:
            indices = sorted(random.sample(range(num_video_frames), self.num_frames))
            frames = frames[indices, :, :, :]
        elif num_video_frames < self.num_frames:
            padding = torch.zeros((self.num_frames - num_video_frames, 3, 224, 224))
            frames = torch.cat((frames, padding), dim=0)

        return frames, label

train_dataset = VideoDataset(dataframe=train_df, num_frames=16)
test_dataset = VideoDataset(dataframe=test_df, num_frames=16)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

for i in train_loader:
  print(i[0].shape)
  break

best_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with tqdm(total=len(train_loader), desc=f"Epoch {epoch+1}/{num_epochs} - Training", unit="batch") as pbar:
        for batch in train_loader:
            inputs, labels = batch
            inputs = inputs.to(device)

            inputs = {'pixel_values': inputs}
            labels = labels.to(device)

            optimizer.zero_grad()

            # Forward pass
            outputs = model(**inputs)
            loss = criterion(outputs.logits, labels)

            # Backward pass
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, preds = torch.max(outputs.logits, 1)
            correct_predictions += torch.sum(preds == labels).item()
            total_samples += labels.size(0)

            pbar.set_postfix({"loss": running_loss / (pbar.n + 1)})
            pbar.update(1)

    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = correct_predictions / total_samples
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}")

    # Save the latest model
    torch.save(model.state_dict(), '/content/drive/MyDrive/Sports Analysis/latest_model.pth')

    model.eval()
    test_loss = 0.0
    correct_predictions_test = 0
    total_samples_test = 0

    with torch.no_grad():
        with tqdm(total=len(test_loader), desc=f"Epoch {epoch+1}/{num_epochs} - Testing", unit="batch") as pbar:
            for batch in test_loader:
                inputs, labels = batch

                inputs = inputs.to(device)

                inputs = {'pixel_values': inputs}
                labels = labels.to(device)

                # Forward pass
                outputs = model(**inputs)
                loss = criterion(outputs.logits, labels)

                test_loss += loss.item()
                _, preds = torch.max(outputs.logits, 1)
                correct_predictions_test += torch.sum(preds == labels).item()
                total_samples_test += labels.size(0)

                pbar.update(1)

        test_loss = test_loss / len(test_loader)
        test_accuracy = correct_predictions_test / total_samples_test
        print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            torch.save(model.state_dict(), '/content/drive/MyDrive/Sports Analysis/best_model.pth')
            print(f"New best model saved with accuracy: {best_accuracy:.4f}")

model.load_state_dict(torch.load('/content/drive/MyDrive/Sports Analysis/latest_model.pth', map_location=device))

model.eval()

"""##For a sample video"""

def preprocess_video(video_path, num_frames=16, frame_size=(224, 224)):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, frame_size)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame)
    cap.release()


    frames = np.array(frames).astype(np.float32) / 255.0
    frames = torch.Tensor(frames).permute(0, 3, 1, 2)


    num_video_frames = frames.shape[0]
    if num_video_frames > num_frames:
        indices = sorted(random.sample(range(num_video_frames), num_frames))
        frames = frames[indices, :, :, :]
    elif num_video_frames < num_frames:
        padding = torch.zeros((num_frames - num_video_frames, 3, 224, 224))
        frames = torch.cat((frames, padding), dim=0)

    frames = frames.unsqueeze(0)
    return {"pixel_values": frames.to(device)}

video_path = '/content/drive/MyDrive/CricShot10 dataset/defense/defense_0073.avi'

inputs = preprocess_video(video_path)

with torch.no_grad():
    outputs = model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)
    predicted_class = torch.argmax(probabilities, dim=1).item()

reverse_label_mapping = {idx: label for label, idx in label_mapping.items()}
predicted_label = reverse_label_mapping[predicted_class]

print(f"Predicted label: {predicted_label}")

"""## Combine with LSTM"""

class VideoMAEWithLSTM(nn.Module):
    def __init__(self, base_model, hidden_size, num_classes, num_lstm_layers=1):
        super(VideoMAEWithLSTM, self).__init__()
        self.base_model = base_model
        self.base_model.classifier = nn.Identity()
        self.lstm = nn.LSTM(input_size=base_model.config.hidden_size, hidden_size=hidden_size, num_layers=num_lstm_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = {'pixel_values': x}

        with torch.no_grad():
            x = self.base_model(**x)['logits']

        lstm_out, _ = self.lstm(x)
        # final_output = lstm_out[:, -1, :]

        output = self.fc(lstm_out)
        return output

hidden_size = 256
num_classes = len(label_mapping)

model_with_lstm = VideoMAEWithLSTM(model, hidden_size, num_classes)
model_with_lstm.to(device)

optimizer = AdamW(model_with_lstm.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    model_with_lstm.train()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with tqdm(total=len(train_loader), desc=f"Epoch {epoch+1}/{num_epochs} - Training", unit="batch") as pbar:
        for batch in train_loader:
            inputs, labels = batch
            inputs = inputs.to(device)

            #inputs = {'pixel_values': inputs}
            labels = labels.to(device)

            optimizer.zero_grad()

            outputs = model_with_lstm(inputs)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct_predictions += torch.sum(preds == labels).item()
            total_samples += labels.size(0)

            pbar.set_postfix({"loss": running_loss / (pbar.n + 1)})
            pbar.update(1)


    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = correct_predictions / total_samples
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}")

    model_with_lstm.eval()
    test_loss = 0.0
    correct_predictions_test = 0
    total_samples_test = 0

    with torch.no_grad():
        with tqdm(total=len(test_loader), desc=f"Epoch {epoch+1}/{num_epochs} - Testing", unit="batch") as pbar:
            for batch in test_loader:
                inputs, labels = batch

                inputs = inputs.to(device)

                # inputs = {'pixel_values': inputs}
                labels = labels.to(device)


                outputs = model_with_lstm(inputs)
                loss = criterion(outputs, labels)


                test_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                correct_predictions_test += torch.sum(preds == labels).item()
                total_samples_test += labels.size(0)

                pbar.update(1)

        test_loss = test_loss / len(test_loader)
        test_accuracy = correct_predictions_test / total_samples_test
        print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

"""## 3D CNN

"""

class CNN3D(nn.Module):
    def __init__(self, num_classes):
        super(CNN3D, self).__init__()

        self.conv1 = nn.Conv3d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.conv3 = nn.Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)
        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.fc1 = nn.Linear(256*3*14*14, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = self.pool3(torch.relu(self.conv3(x)))
        # print(x.shape)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    model.to(device)

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')

        validate_model(model, val_loader)

def validate_model(model, val_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Validation Accuracy: {100 * correct / total}%')

def test_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.cuda(), labels.cuda()
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

train_dataset = VideoDataset(train_df)
val_dataset = VideoDataset(val_df)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)


num_classes = len(df['label'].unique())
model = CNN3D(num_classes=num_classes).to(device)


train_model(model, train_loader, val_loader)

sampled_df = df.sample(n=100, random_state=42)

train_df = sampled_df.iloc[:80]
test_df = sampled_df.iloc[80:]

train_dataset = VideoDataset(train_df, label_mapping, num_frames=30)
test_dataset = VideoDataset(test_df, label_mapping, num_frames=30)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

model = CNN3D(num_classes=num_classes).cuda()

train_model(model, train_loader, test_loader, num_epochs=5)

sampled_df = df.sample(n=200, random_state=42)

train_df, temp_df = train_test_split(sampled_df, test_size=0.2, random_state=42)

val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

train_dataset = VideoDataset(train_df, num_frames=30)
val_dataset = VideoDataset(val_df, num_frames=30)
test_dataset = VideoDataset(test_df, num_frames=30)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

model = CNN3D(num_classes=num_classes).cuda()

train_model(model, train_loader, val_loader, num_epochs=10)
test_model(model,test_loader)

"""## 2D CNN + LSTM"""

import torch
import torch.nn as nn
import torchvision.models as models

class CNNFeatureExtractor(nn.Module):
    def __init__(self):
        super(CNNFeatureExtractor, self).__init__()
        self.resnet = models.resnet18(pretrained=True)
        self.resnet.fc = nn.Identity()

    def forward(self, x):
        with torch.no_grad():
            x = self.resnet(x)
        return x

class CNNLSTM(nn.Module):
    def __init__(self, cnn, hidden_size, num_classes):
        super(CNNLSTM, self).__init__()
        self.cnn = cnn
        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=2, batch_first=True, dropout=0.5)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        batch_size, num_frames, channels, height, width = x.shape
        assert channels == 3
        cnn_features = []

        for i in range(num_frames):
            frame = x[:, i, :, :, :]
            frame_features = self.cnn(frame)
            cnn_features.append(frame_features)

        cnn_features = torch.stack(cnn_features, dim=1)
        lstm_out, _ = self.lstm(cnn_features)
        out = self.fc(lstm_out[:, -1, :])
        return out

# Initialize models
cnn = CNNFeatureExtractor()
model = CNNLSTM(cnn=cnn, hidden_size=256, num_classes=num_classes).cuda()

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
def train_model(model, train_loader, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        for inputs, labels in train_loader:
            inputs, labels = inputs.cuda(), labels.cuda()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f'Epoch {epoch+1}, Loss: {loss.item()}')

sampled_df = df.sample(n=100, random_state=42)

train_df = sampled_df.iloc[:80]
test_df = sampled_df.iloc[80:]

train_dataset = VideoDataset(train_df, label_mapping, num_frames=30)
test_dataset = VideoDataset(test_df, label_mapping, num_frames=30)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

model = CNNLSTM(cnn=cnn, hidden_size=256, num_classes=num_classes).cuda()

train_model(model, train_loader, num_epochs=10)

validate_model(model, test_loader)

import torch
import torch.nn as nn
import torchvision.models as models

class ResNetFeatureExtractor(nn.Module):
    def __init__(self):
        super(ResNetFeatureExtractor, self).__init__()
        self.resnet = models.resnet18(pretrained=True)
        self.resnet.fc = nn.Identity()

    def forward(self, x):
        with torch.no_grad():
            x = self.resnet(x)
        return x

class ResNetLSTM(nn.Module):
    def __init__(self, cnn, hidden_size, num_classes):
        super(ResNetLSTM, self).__init__()
        self.cnn = cnn
        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=2, batch_first=True, dropout=0.5)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        batch_size, num_frames, channels, height, width = x.shape
        cnn_features = []

        for i in range(num_frames):
            frame = x[:, i, :, :, :]
            frame_features = self.cnn(frame)
            cnn_features.append(frame_features)

        cnn_features = torch.stack(cnn_features, dim=1)
        lstm_out, _ = self.lstm(cnn_features)
        out = self.fc(lstm_out[:, -1, :])
        return out

cnn = ResNetFeatureExtractor()
model = ResNetLSTM(cnn=cnn, hidden_size=256, num_classes=num_classes).cuda()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_model(model, train_loader, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        for inputs, labels in train_loader:
            inputs, labels = inputs.cuda(), labels.cuda()

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')

sampled_df = df.sample(n=100, random_state=42)

train_df = sampled_df.iloc[:80]
test_df = sampled_df.iloc[80:]

train_dataset = VideoDataset(train_df, label_mapping, num_frames=30)
test_dataset = VideoDataset(test_df, label_mapping, num_frames=30)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

model = ResNetLSTM(cnn=cnn, hidden_size=256, num_classes=num_classes).cuda()

train_model(model, train_loader, num_epochs=10)

validate_model(model, test_loader)

"""VideoMAE"""

from torch.utils.data import DataLoader

# Assuming you have your dataset split into train_df, val_df, and test_df
# Initialize datasets
train_dataset = VideoDataset(train_df, label_mapping, num_frames=30)
val_dataset = VideoDataset(val_df, label_mapping, num_frames=30)
test_dataset = VideoDataset(test_df, label_mapping, num_frames=30)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)

from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor
import torch
import torch.nn as nn

model = VideoMAEForVideoClassification.from_pretrained("MCG-NJU/videomae-base")
processor = VideoMAEImageProcessor.from_pretrained("MCG-NJU/videomae-base")

num_features = model.classifier.in_features
model.classifier = nn.Sequential(
    nn.Linear(num_features, 512),
    nn.ReLU(),
    nn.Linear(512, 10)
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

def preprocess_videos(videos):
    pixel_values = processor(videos, return_tensors="pt").pixel_values
    return pixel_values.to(device)

def train_model(model, train_loader, val_loader, epochs, criterion, optimizer):
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0

        for videos, labels in train_loader:
            videos, labels = videos.to(device), labels.to(device)
            videos = preprocess_videos(videos) n
            optimizer.zero_grad()
            outputs = model(videos)
            loss = criterion(outputs.logits, labels)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')

        validate_model(model, val_loader)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

train_model(model, train_loader, val_loader, epochs=10, criterion=criterion, optimizer=optimizer)

def validate_model(model, val_loader):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for videos, labels in val_loader:
            videos, labels = videos.to(device), labels.to(device)
            outputs = model(videos)
            loss = criterion(outputs.logits, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs.logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_accuracy = 100 * correct / total
    avg_val_loss = val_loss / len(val_loader)
    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')

import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Create figure and axis
fig, ax = plt.subplots(figsize=(12, 8))

# Define the flowchart components
components = [
    {"label": "Load Video", "x": 0.1, "y": 0.8},
    {"label": "Extract Frames", "x": 0.4, "y": 0.8},
    {"label": "Resize Frames\n(224x224)", "x": 0.7, "y": 0.8},
    {"label": "Normalize Pixel\nValues (0-1)", "x": 0.4, "y": 0.6},
    {"label": "Sample or Pad\nFrames to 30", "x": 0.4, "y": 0.4},
    {"label": "Convert to Tensor\n(C, T, H, W)", "x": 0.4, "y": 0.2},
    {"label": "Encode Labels", "x": 0.7, "y": 0.2},
]

# Draw the components
for comp in components:
    rect = patches.FancyBboxPatch(
        (comp["x"], comp["y"]), 0.25, 0.1,
        boxstyle="round,pad=0.02", edgecolor="black", facecolor="lightblue", linewidth=1.5
    )
    ax.add_patch(rect)
    ax.text(comp["x"] + 0.125, comp["y"] + 0.05, comp["label"], ha="center", va="center", fontsize=10)

# Define connections
connections = [
    {"start": (0.325, 0.85), "end": (0.4, 0.85)},  # Load -> Extract Frames
    {"start": (0.625, 0.85), "end": (0.7, 0.85)},  # Extract Frames -> Resize
    {"start": (0.525, 0.75), "end": (0.525, 0.65)},  # Resize -> Normalize
    {"start": (0.525, 0.55), "end": (0.525, 0.45)},  # Normalize -> Sample/Pad
    {"start": (0.525, 0.35), "end": (0.525, 0.25)},  # Sample/Pad -> Convert to Tensor
    {"start": (0.525, 0.25), "end": (0.7, 0.25)},   # Convert to Tensor -> Encode Labels
]

# Draw arrows
for conn in connections:
    ax.annotate("", xy=conn["end"], xytext=conn["start"],
                arrowprops=dict(arrowstyle="->", color="black", lw=1.5))

# Remove axes and set title
ax.axis("off")
ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
plt.title("Video Preprocessing Pipeline", fontsize=14, fontweight="bold")
plt.show()